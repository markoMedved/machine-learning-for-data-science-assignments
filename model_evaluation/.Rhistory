# Shuffle
folds[[i]] <- folds[[i]][sample(nrow(folds[[i]])), ]
}
return(folds)
}
# Log loss function
log_loss <- function(pred_distr, targets) {
epsilon <- 1e-15
log_scores <- numeric(nrow(targets))
for(i in 1:nrow(targets)){
prob <- pred_distr[i, ]
log_scores[i] <- -log(max(as.numeric(prob[targets[i, 1]]), epsilon))
}
loss <- mean(log_scores)
SE = bootstrap(log_scores, mean)
return(list(log_loss = loss, se = SE))
}
accuracy <- function(pred_distr, targets) {
preds <- apply(pred_distr, 1, function(prob) names(prob)[which.max(prob)])
corr_pred = preds == targets[, 1]
acc <- mean(corr_pred)
se <- bootstrap(corr_pred, mean)
return(list(accurcy = acc, SE = se))
}
# k = ?
k = 10
target_col <- "ShotType"
folds <- stratified_sampling(df, k, target_col)
# Cross validation
bsln_accs <- c()
bsln_acc_ses <- c()
bsln_log_scores <- c()
bsln_log_ses<- c()
log_reg_accs<- c()
log_reg_acc_ses <- c()
log_reg_log_scores <- c()
log_reg_log_ses <- c()
for(i in seq_along(folds)){
# Select the training and test folds
test <- folds[[i]]
train <- folds[-i]
train <- bind_rows(train)
table(train$Competition)/(k-1)
table(test$Competition)
# Get the targets
targets <- test[target_col]
# Evaluate the models
# Baseline
bsln <- baseline_classifier(train[target_col])
# Repeat the distribution fo each target("Prediction")
bsln_pred <- bsln[rep(1, nrow(targets)), ]
# Get the log loss and accuracy
log_loss_bsln <- log_loss(bsln_pred, targets)
bsln_log_scores[i] <- log_loss_bsln$log_loss
bsln_log_ses[i] <- log_loss_bsln$se
acc_bsln <- accuracy(bsln_pred, targets)
bsln_accs[i] <- acc_bsln$accurcy
bsln_acc_ses[i] <- acc_bsln$SE
# Logistic regression
log_reg <- multinom(as.factor(train[[target_col]] ) ~ ., data = train[, colnames(train) != target_col])
# Predictions
log_reg_pred <- predict(log_reg, test, type="probs")
# Acc
acc_log_reg <- accuracy(log_reg_pred, targets)
log_reg_accs[i] <- acc_log_reg$accurcy
log_reg_acc_ses[i] <- acc_log_reg$SE
# Log score
log_loss_log_reg <- log_loss(log_reg_pred, targets)
log_reg_log_scores[i] <- log_loss_log_reg$log_loss
log_reg_log_ses[i] <- log_loss_log_reg$se
}
print(bsln_log_scores)
print(bsln_accs)
print(log_reg_log_scores)
print(log_reg_accs)
# SVM using linear kernel, tuning
for(i in seq_along(folds)){
# Select the training and test folds
test <- folds[[i]]
train <- folds[-i]
train <- bind_rows(train)
table(train$Competition)/(k-1)
table(test$Competition)
# Get the targets
targets <- test[target_col]
# Also get the train targets here
train_targets <- train[target_col]
# Optimizing the training fold performance
best_C <- 1e-3
best_log_loss <- 1e10
for (C in c(1e-3,1e-2,1e-1,1,10,100)){
svm_model <- svm(as.factor(train[[target_col]] ) ~ .,
data = train[, colnames(train) != target_col]
,kernel = "linear",
probability = TRUE,
cost = C)
svm_pred <- predict(svm_model, train, probability = TRUE)
svm_pred <- attr(svm_pred, "probabilities")
# Uses log-loss for evaluating as criteria for train-fold performance
log_loss_svm <- log_loss(svm_pred, train_targets)
if(log_loss_svm < best_log_loss){
best_log_loss <- log_loss_svm
best_C <- C
}
print(C)
print(log_loss_svm)
}
svm_model <- svm(as.factor(train[[target_col]] ) ~ .,
data = train[, colnames(train) != target_col]
,kernel = "linear",
probability = TRUE,
cost = best_C)
svm_pred <- predict(svm_model, test, probability = TRUE)
svm_pred <- attr(svm_pred, "probabilities")
# Acc
acc_svm <- accuracy(svm_pred, targets)
# Log score
log_loss_svm <- log_loss(svm_pred, targets)
print(acc_svm)
print(log_loss_svm)
}
# Libraries
#install.packages("dplyr")
# install.packages("e1071")
library(dplyr)
library(nnet)
library(e1071)
library(class)
# Read the csv file to a data frame
df <- read.csv("dataset.csv", sep=";", header=TRUE)
# drop duplicated rows
df <- distinct(df)
X <- df[, -1]
y <- df$ShotType
# Define the baseline classifier
baseline_classifier <- function(y){
bsln <- data.frame(table(y)/length(y[,1]))
bsln <- as.data.frame(t(setNames(bsln$Freq, bsln$ShotType)))
return(bsln)
}
# Bootstrap algorithm
bootstrap <- function(x, f, m = 1000, seed = 42){
theta <- c()
set.seed(seed)
for(i in 1:m){
FB <- sample(x, length(x), replace = TRUE)
curr <- f(FB)
theta <- c(theta, curr)
}
return (SE = sd(theta))
}
# Other models -> Logistic regression and SVM
# Use stratified sampling, because the data set is imbalanced, and it's representative of the DGP
stratified_sampling <- function(df, k, target_col, seed = 42){
set.seed(seed)
# Split the data set by classes
strata <- split(df, df[,target_col])
strata_tmp <- strata
# lists for storing all the samples
folds <- list()
# Implement stratified sampling
for(i in  1:k){
current_samp <- data.frame()
for (j in seq_along(strata)) {
len_smp <- as.integer(nrow(strata_tmp[[j]]) / k)
rows <- strata[[j]][sample(nrow(strata[[j]]), len_smp, replace = FALSE), ]
current_samp <- rbind(current_samp, rows)
# Remove the sampled data
strata[[j]] <- anti_join(strata[[j]], rows, by = colnames(df))
}
folds[[i]] <- current_samp
# Shuffle
folds[[i]] <- folds[[i]][sample(nrow(folds[[i]])), ]
}
return(folds)
}
# Log loss function
log_loss <- function(pred_distr, targets) {
epsilon <- 1e-15
log_scores <- numeric(nrow(targets))
for(i in 1:nrow(targets)){
prob <- pred_distr[i, ]
log_scores[i] <- -log(max(as.numeric(prob[targets[i, 1]]), epsilon))
}
loss <- mean(log_scores)
SE = bootstrap(log_scores, mean)
return(list(log_loss = loss, se = SE))
}
accuracy <- function(pred_distr, targets) {
preds <- apply(pred_distr, 1, function(prob) names(prob)[which.max(prob)])
corr_pred = preds == targets[, 1]
acc <- mean(corr_pred)
se <- bootstrap(corr_pred, mean)
return(list(accurcy = acc, SE = se))
}
# k = ?
k = 10
target_col <- "ShotType"
folds <- stratified_sampling(df, k, target_col)
# Cross validation
bsln_accs <- c()
bsln_acc_ses <- c()
bsln_log_scores <- c()
bsln_log_ses<- c()
log_reg_accs<- c()
log_reg_acc_ses <- c()
log_reg_log_scores <- c()
log_reg_log_ses <- c()
for(i in seq_along(folds)){
# Select the training and test folds
test <- folds[[i]]
train <- folds[-i]
train <- bind_rows(train)
table(train$Competition)/(k-1)
table(test$Competition)
# Get the targets
targets <- test[target_col]
# Evaluate the models
# Baseline
bsln <- baseline_classifier(train[target_col])
# Repeat the distribution fo each target("Prediction")
bsln_pred <- bsln[rep(1, nrow(targets)), ]
# Get the log loss and accuracy
log_loss_bsln <- log_loss(bsln_pred, targets)
bsln_log_scores[i] <- log_loss_bsln$log_loss
bsln_log_ses[i] <- log_loss_bsln$se
acc_bsln <- accuracy(bsln_pred, targets)
bsln_accs[i] <- acc_bsln$accurcy
bsln_acc_ses[i] <- acc_bsln$SE
# Logistic regression
log_reg <- multinom(as.factor(train[[target_col]] ) ~ ., data = train[, colnames(train) != target_col])
# Predictions
log_reg_pred <- predict(log_reg, test, type="probs")
# Acc
acc_log_reg <- accuracy(log_reg_pred, targets)
log_reg_accs[i] <- acc_log_reg$accurcy
log_reg_acc_ses[i] <- acc_log_reg$SE
# Log score
log_loss_log_reg <- log_loss(log_reg_pred, targets)
log_reg_log_scores[i] <- log_loss_log_reg$log_loss
log_reg_log_ses[i] <- log_loss_log_reg$se
}
print(bsln_log_scores)
print(bsln_accs)
print(log_reg_log_scores)
print(log_reg_accs)
# SVM using linear kernel, tuning
for(i in seq_along(folds)){
# Select the training and test folds
test <- folds[[i]]
train <- folds[-i]
train <- bind_rows(train)
table(train$Competition)/(k-1)
table(test$Competition)
# Get the targets
targets <- test[target_col]
# Also get the train targets here
train_targets <- train[target_col]
# Optimizing the training fold performance
best_C <- 1e-3
best_log_loss <- 1e10
for (C in c(1e-3,1e-2,1e-1,1,10,100)){
svm_model <- svm(as.factor(train[[target_col]] ) ~ .,
data = train[, colnames(train) != target_col]
,kernel = "linear",
probability = TRUE,
cost = C)
svm_pred <- predict(svm_model, train, probability = TRUE)
svm_pred <- attr(svm_pred, "probabilities")
# Uses log-loss for evaluating as criteria for train-fold performance
log_loss_svm <- log_loss(svm_pred, train_targets)
if(log_loss_svm < best_log_loss){
best_log_loss <- log_loss_svm
best_C <- C
}
print(C)
print(log_loss_svm)
}
svm_model <- svm(as.factor(train[[target_col]] ) ~ .,
data = train[, colnames(train) != target_col]
,kernel = "linear",
probability = TRUE,
cost = best_C)
svm_pred <- predict(svm_model, test, probability = TRUE)
svm_pred <- attr(svm_pred, "probabilities")
# Acc
acc_svm <- accuracy(svm_pred, targets)
# Log score
log_loss_svm <- log_loss(svm_pred, targets)
print(acc_svm)
print(log_loss_svm)
}
# Libraries
#install.packages("dplyr")
# install.packages("e1071")
library(dplyr)
library(nnet)
library(e1071)
library(class)
# Read the csv file to a data frame
df <- read.csv("dataset.csv", sep=";", header=TRUE)
# drop duplicated rows
df <- distinct(df)
X <- df[, -1]
y <- df$ShotType
# Define the baseline classifier
baseline_classifier <- function(y){
bsln <- data.frame(table(y)/length(y[,1]))
bsln <- as.data.frame(t(setNames(bsln$Freq, bsln$ShotType)))
return(bsln)
}
# Bootstrap algorithm
bootstrap <- function(x, f, m = 1000, seed = 42){
theta <- c()
set.seed(seed)
for(i in 1:m){
FB <- sample(x, length(x), replace = TRUE)
curr <- f(FB)
theta <- c(theta, curr)
}
return (SE = sd(theta))
}
# Other models -> Logistic regression and SVM
# Use stratified sampling, because the data set is imbalanced, and it's representative of the DGP
stratified_sampling <- function(df, k, target_col, seed = 42){
set.seed(seed)
# Split the data set by classes
strata <- split(df, df[,target_col])
strata_tmp <- strata
# lists for storing all the samples
folds <- list()
# Implement stratified sampling
for(i in  1:k){
current_samp <- data.frame()
for (j in seq_along(strata)) {
len_smp <- as.integer(nrow(strata_tmp[[j]]) / k)
rows <- strata[[j]][sample(nrow(strata[[j]]), len_smp, replace = FALSE), ]
current_samp <- rbind(current_samp, rows)
# Remove the sampled data
strata[[j]] <- anti_join(strata[[j]], rows, by = colnames(df))
}
folds[[i]] <- current_samp
# Shuffle
folds[[i]] <- folds[[i]][sample(nrow(folds[[i]])), ]
}
return(folds)
}
# Log loss function
log_loss <- function(pred_distr, targets) {
epsilon <- 1e-15
log_scores <- numeric(nrow(targets))
for(i in 1:nrow(targets)){
prob <- pred_distr[i, ]
log_scores[i] <- -log(max(as.numeric(prob[targets[i, 1]]), epsilon))
}
loss <- mean(log_scores)
SE = bootstrap(log_scores, mean)
return(list(log_loss = loss, se = SE))
}
accuracy <- function(pred_distr, targets) {
preds <- apply(pred_distr, 1, function(prob) names(prob)[which.max(prob)])
corr_pred = preds == targets[, 1]
acc <- mean(corr_pred)
se <- bootstrap(corr_pred, mean)
return(list(accurcy = acc, SE = se))
}
# k = ?
k = 10
target_col <- "ShotType"
folds <- stratified_sampling(df, k, target_col)
# Cross validation
bsln_accs <- c()
bsln_acc_ses <- c()
bsln_log_scores <- c()
bsln_log_ses<- c()
log_reg_accs<- c()
log_reg_acc_ses <- c()
log_reg_log_scores <- c()
log_reg_log_ses <- c()
for(i in seq_along(folds)){
# Select the training and test folds
test <- folds[[i]]
train <- folds[-i]
train <- bind_rows(train)
table(train$Competition)/(k-1)
table(test$Competition)
# Get the targets
targets <- test[target_col]
# Evaluate the models
# Baseline
bsln <- baseline_classifier(train[target_col])
# Repeat the distribution fo each target("Prediction")
bsln_pred <- bsln[rep(1, nrow(targets)), ]
# Get the log loss and accuracy
log_loss_bsln <- log_loss(bsln_pred, targets)
bsln_log_scores[i] <- log_loss_bsln$log_loss
bsln_log_ses[i] <- log_loss_bsln$se
acc_bsln <- accuracy(bsln_pred, targets)
bsln_accs[i] <- acc_bsln$accurcy
bsln_acc_ses[i] <- acc_bsln$SE
# Logistic regression
log_reg <- multinom(as.factor(train[[target_col]] ) ~ ., data = train[, colnames(train) != target_col])
# Predictions
log_reg_pred <- predict(log_reg, test, type="probs")
# Acc
acc_log_reg <- accuracy(log_reg_pred, targets)
log_reg_accs[i] <- acc_log_reg$accurcy
log_reg_acc_ses[i] <- acc_log_reg$SE
# Log score
log_loss_log_reg <- log_loss(log_reg_pred, targets)
log_reg_log_scores[i] <- log_loss_log_reg$log_loss
log_reg_log_ses[i] <- log_loss_log_reg$se
}
print(bsln_log_scores)
print(bsln_accs)
print(log_reg_log_scores)
print(log_reg_accs)
# SVM using linear kernel, tuning
for(i in seq_along(folds)){
# Select the training and test folds
test <- folds[[i]]
train <- folds[-i]
train <- bind_rows(train)
table(train$Competition)/(k-1)
table(test$Competition)
# Get the targets
targets <- test[target_col]
# Also get the train targets here
train_targets <- train[target_col]
# Optimizing the training fold performance
best_C <- 1e-3
best_log_loss <- 1e10
for (C in c(1e-3,1e-2,1e-1,1,10)){
svm_model <- svm(as.factor(train[[target_col]] ) ~ .,
data = train[, colnames(train) != target_col]
,kernel = "linear",
probability = TRUE,
cost = C)
svm_pred <- predict(svm_model, train, probability = TRUE)
svm_pred <- attr(svm_pred, "probabilities")
# Uses log-loss for evaluating as criteria for train-fold performance
log_loss_svm <- as.numeric(log_loss(svm_pred, train_targets)[1])
print(C)
print(best_log_loss)
print(log_loss_svm)
if(log_loss_svm < best_log_loss){
best_log_loss <- log_loss_svm
best_C <- C
}
}
svm_model <- svm(as.factor(train[[target_col]] ) ~ .,
data = train[, colnames(train) != target_col]
,kernel = "linear",
probability = TRUE,
cost = best_C)
svm_pred <- predict(svm_model, test, probability = TRUE)
svm_pred <- attr(svm_pred, "probabilities")
# Acc
acc_svm <- accuracy(svm_pred, targets)
# Log score
log_loss_svm <- log_loss(svm_pred, targets)
print(acc_svm)
print(log_loss_svm)
}
# SVM using linear kernel, tuning
for(i in seq_along(folds)){
# Select the training and test folds
test <- folds[[i]]
train <- folds[-i]
train <- bind_rows(train)
table(train$Competition)/(k-1)
table(test$Competition)
# Get the targets
targets <- test[target_col]
# Also get the train targets here
train_targets <- train[target_col]
# Optimizing the training fold performance
best_C <- 1e-3
best_log_loss <- 1e10
for (C in c(1e-3,1e-2,1e-1,1,10)){
svm_model <- svm(as.factor(train[[target_col]] ) ~ .,
data = train[, colnames(train) != target_col]
,kernel = "linear",
probability = TRUE,
cost = C)
svm_pred <- predict(svm_model, train, probability = TRUE)
svm_pred <- attr(svm_pred, "probabilities")
print(accuracy(svm_pred, train_targets))
# Uses log-loss for evaluating as criteria for train-fold performance
log_loss_svm <- as.numeric(log_loss(svm_pred, train_targets)[1])
print(C)
print(best_log_loss)
print(log_loss_svm)
if(log_loss_svm < best_log_loss){
best_log_loss <- log_loss_svm
best_C <- C
}
}
svm_model <- svm(as.factor(train[[target_col]] ) ~ .,
data = train[, colnames(train) != target_col]
,kernel = "linear",
probability = TRUE,
cost = best_C)
svm_pred <- predict(svm_model, test, probability = TRUE)
svm_pred <- attr(svm_pred, "probabilities")
# Acc
acc_svm <- accuracy(svm_pred, targets)
# Log score
log_loss_svm <- log_loss(svm_pred, targets)
print(acc_svm)
print(log_loss_svm)
}
